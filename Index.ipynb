{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<html>\n",
    "<p style=\"font-size:32px\"><strong>Classical Machine Learning</strong></p>\n",
    "</html>\n",
    "\n",
    "<html>\n",
    "<p style=\"font-size:26px\"><strong>Week 0</strong></p>\n",
    "</html>\n",
    " \n",
    "\n",
    "**Plan**\n",
    "- Setting up your learning and programming environment\n",
    "\n",
    "\n",
    "**Getting started**\n",
    "- [Setting up your ML environment](Setup_NYU.ipynb)\n",
    "    - [Choosing an ML environment](Choosing_an_ML_Environment_NYU.ipynb)\n",
    "- [Quick intro to the tools](Getting_Started.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 1\n",
    "**Plan**\n",
    "- Motivate Machine Learning\n",
    "- Introduce notation used throughout course\n",
    "- Plan for initial lectures\n",
    "    - *What*: Introduce, motivate a model\n",
    "    - *How*:  How to use a model: function signature, code (API)\n",
    "    - *Why*:  Mathematical basis -- enhance understanding and ability to improve results\n",
    "\n",
    "        \n",
    "- [Course Overview](Course_overview_NYU.ipynb)\n",
    "\n",
    "- [Machine Learning: Overview](ML_Overview.ipynb)\n",
    "- [Intro to Classical ML](Intro_Classical_ML.ipynb)\n",
    "\n",
    "**Week 2 (early start)**\n",
    "\n",
    "**Plan**\n",
    "- Introduce a model for the Regression task: Linear Regression\n",
    "- Introduce the Recipe for Machine Learning: detailed steps to problem solving\n",
    "\n",
    "\n",
    "- [Our first model: Linear Regression (Overview)](Linear_Regression_Overview.ipynb)\n",
    "- A *process* for Machine Learning\n",
    "    - Go through the methodical, multi-step process\n",
    "        - Quick first pass, followed by Deeper Dives\n",
    "    - This will be a code-heavy notebook !\n",
    "    - Illustrate Pandas, Jupyter, etc\n",
    "    \n",
    "**The Recipe for Machine Learning, illustrated with the Linear Regression model**  \n",
    "- [Recipe for Machine Learning: Overview](Recipe_Overview.ipynb)\n",
    "    - [Linked notebook](Recipe_for_ML.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 2\n",
    "**Plan**\n",
    "\n",
    "We will continue learning the Recipe for Machine Learning, illustrated by introducing the Linear Regression model for solving Regression tasks.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Recipe for Machine Learning</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/W1_L3_S4_ML_Process.png\" width=\"100%\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "**Recipe for Machine Learning/Linear Regression** (continued)\n",
    "\n",
    "- We continue with Step 2 of the Recipe: [Exploratory Data Analysis](Recipe_Overview.ipynb#Exploratory-Data-Analysis)\n",
    "    - [Linked notebook](Recipe_for_ML.ipynb)\n",
    "- [Regularization: a way to combat overfitting](Bias_and_Variance.ipynb#Regularization:-reducing-overfitting)\n",
    "    \n",
    "    \n",
    "- [Linear Regression: Loss Function](Linear_Regression_Loss_Function.ipynb)\n",
    "\n",
    "**Transformations**\n",
    " - [Prepare Data: Intro to Transformations](Prepare_data_Overview.ipynb)\n",
    " \n",
    "**Deeper dives**\n",
    "- Iterative improvement\n",
    "    - [When to stop: Bias and Variance](Bias_and_Variance.ipynb)\n",
    "        - Regularization\n",
    "- [Fine tuning techniques](Fine_tuning.ipynb)\n",
    "\n",
    "**Week 3 (early start)**\n",
    "\n",
    "**Plan**\n",
    "- Introduce a model for the Classification task: Logistic Regression\n",
    "\n",
    "**Classification intro**\n",
    "- [Classification: Overview](Classification_Overview.ipynb)\n",
    "- [Classification and Categorical Variables](Classification_Notebook_Overview.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 3\n",
    "\n",
    "**Regression wrap-up**\n",
    "\n",
    "A few missing items/clarifications from last week\n",
    "- Use linear regression for hedging (clarification).  I gave the linear regression example\n",
    "$$\n",
    "r_\\text{AAPL} \\approx \\Theta_0 + \\Theta_1 * r_\\text{SPX}\n",
    "$$\n",
    "    - The return of AAPL is $\\Theta_1$ times as much as the return of the index SPX\n",
    "    - So if I hold \\\\$1 of AAPL long, I would short $\\Theta_1$ of SPX in order to neutralize market exposure\n",
    "    - Often assume $\\Theta_0 = 0$\n",
    "    - Remember: the fit involves an \"error\"\n",
    "    $$\n",
    "r_\\text{AAPL} = \\Theta_0 + \\Theta_1 * r_\\text{SPX} + \\epsilon\n",
    "$$\n",
    "    - The hedged portfolio returns $\\epsilon$\n",
    "    - If, through my modeling, I conclude that $\\epsilon > 0$, I can profit regardless of the direction of SPX\n",
    "\n",
    "- Where do regression errors come from ?\n",
    "    - Measurement error, noise\n",
    "    - Missing feature\n",
    "    \n",
    "\n",
    "**Plan**\n",
    "- We continue our Introduction of a model for the Classification task: Logistic Regression\n",
    "- How to deal with Categorical (non-numeric) variables\n",
    "    - classification target\n",
    "    - features\n",
    "\n",
    "<img src='images/class_overview_prob_lines.jpg'>\n",
    "\n",
    "**Classification intro, continued from last week**\n",
    "- [Classification and Categorical Variables](Classification_Notebook_Overview.ipynb#Recipe-Step-B:-Exploratory-Data-Analysis-(EDA))\n",
    "    - [linked notebook](Classification_and_Non_Numerical_Data.ipynb)\n",
    "\n",
    "**Classification, continued**\n",
    "- [Multinomial Classification](Multinomial_Classification.ipynb)\n",
    "\n",
    "**Deeper dives**\n",
    "- [Log odds](Classification_Log_Odds.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4\n",
    "\n",
    "**The Recipe**\n",
    "\n",
    "I highly recommend reviewing and understanding\n",
    "the [Geron notebook](external/handson-ml2/02_end_to_end_machine_learning_project.ipynb)\n",
    "on all steps of the Recipe.\n",
    "\n",
    "**Plan**\n",
    "- Classsification and Categorical variables wrapup\n",
    "    - Baseline models: a baseline model for classification\n",
    "    - OHE and Linear Regression: The Dummy Variable Trap\n",
    "- Error Analysis\n",
    "\n",
    "**Classsification and Categorical variables wrapup**\n",
    "- [Classification Loss Function](Classification_Loss_Function.ipynb)\n",
    "- [Baseline model for Classification](Classification_Baseline_Model.ipynb)\n",
    "- [Dummy variable trap](Dummy_Variable_Trap.ipynb)\n",
    "\n",
    "**This week**\n",
    "\n",
    "Good news\n",
    "- You now know two main tasks in Supervised Learning\n",
    "    - Regression, Classification\n",
    "- You now know how to use virtually every model in `sklearn`\n",
    "    - Consistent API\n",
    "        - `fit`, `transform`, `predict`\n",
    "- You survived the \"sprint\" to get you up and running with ML\n",
    "- You know the *mechanical process* to implement transformations: Pipelines\n",
    "    \n",
    "**Plan**\n",
    "- Error Analysis\n",
    "    - We explain Error Analysis for the Classification Task, with a detailed example\n",
    "    - How Training Loss can be improved\n",
    "- Transformations, continued\n",
    "    - One of the most important parts of the Recipe: transforming raw data into something that tells a story\n",
    "- Loss functions\n",
    "    - We look at the mathematical logic behind loss functions\n",
    "\n",
    "**Error Analysis**\n",
    "- [Error Analysis](Error_Analysis_Overview.ipynb)\n",
    "    - [linked notebook](Error_Analysis.ipynb)\n",
    "        - Summary statistics\n",
    "        - Conditional statistics\n",
    "    - [Worked example](Error_Analysis_MNIST.ipynb)\n",
    "\n",
    "- [Loss Analysis: Using training loss to improve models](Training_Loss.ipynb)\n",
    " \n",
    "**Transformations**\n",
    "- [Becoming a successful Data Scientist](Becoming_a_successful_Data_Scientist.ipynb)\n",
    "- [Transformations: overview](Transformations_Overview.ipynb)\n",
    "    - linked notebooks:\n",
    "        - [Transformations: adding a missing feature](Transformations_Missing_Features.ipynb)\n",
    "        - [Transformations: scaling](Transformations_Scaling.ipynb)\n",
    "        - [Other Transformations](Transformations_Other.ipynb)\n",
    "        - [Prepare Data: Intro to Transformations](Prepare_data_Overview.ipynb)\n",
    "        \n",
    "\n",
    "**Imbalanced data**\n",
    "- [Imbalanced data](Imbalanced_Data.ipynb)\n",
    "\n",
    "\n",
    "**Loss functions: mathematical basis**\n",
    "- [Entropy, Cross Entropy, and KL Divergence](Entropy_Cross_Entropy_KL_Divergence.ipynb)\n",
    "- [Loss functions: the math](Loss_functions.ipynb)\n",
    "    - Maximum likelihood\n",
    "    - Preview: custom loss functions and Deep Learning\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 5\n",
    "\n",
    "**Loss functions: mathematical basis**\n",
    "- [Entropy, Cross Entropy, and KL Divergence](Entropy_Cross_Entropy_KL_Divergence.ipynb)\n",
    "- [Loss functions: the math](Loss_functions.ipynb)\n",
    "    - Maximum likelihood\n",
    "    - Preview: custom loss functions and Deep Learning\n",
    "   \n",
    "**Plan**\n",
    "- More models: Decision Trees, Naive Bayes\n",
    "    - Different flavor: more procedural, less mathematical\n",
    "    - Decision Trees: a model with *non-linear* boundaries\n",
    "- Ensembles\n",
    "    - Bagging and Boosting\n",
    "    - Random Forests\n",
    "- Naive Bayes: a simple but effective model\n",
    "\n",
    "**Decision Trees, Ensembles**\n",
    "\n",
    "- [Decision Trees: Overview](Decision_Trees_Overview.ipynb)\n",
    "- [Decision Trees](Decision_Trees_Notebook_Overview.ipynb)\n",
    "    - [linked notebook](Decision_Trees.ipynb)\n",
    "- [Trees, Forests, Ensembles](Ensembles.ipynb)\n",
    "\n",
    "**Naive Bayes**\n",
    "- [Naive Bayes](Naive_Bayes.ipynb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 6   \n",
    "\n",
    "**Plan**\n",
    "- We finish up Naive Bayes\n",
    "- Support Vector Classifiers: a classifier with an interesting twist \n",
    "- Unsupervised Learning: Principal Compoments\n",
    "\n",
    "\n",
    "**Naive Bayes (continued)**\n",
    "- [Naive Bayes](Naive_Bayes.ipynb#Features-with-continuous-rather-than-discrete-values)\n",
    "\n",
    "**Unsupervised Learning: PCA**\n",
    "- [Unsupervised Learning: Overview](Unsupervised_Overview.ipynb)\n",
    "- [PCA Notebook Overview](Unsupervised_Notebook_Overview.ipynb)\n",
    "    - [linked notebook](Unsupervised.ipynb)\n",
    "- [PCA in Finance](PCA_Yield_Curve_Intro.ipynb)\n",
    "  \n",
    "**Support Vector Classifiers**\n",
    "- [Support Vector Machines: Overview](SVM_Overview.ipynb)\n",
    "- [SVC Loss function](SVM_Hinge_Loss.ipynb)\n",
    "- [SVC: Large Margin Classification](SVM_Large_Margin.ipynb)  \n",
    "- [SVM: Kernel Transformations](SVM_Kernel_Functions.ipynb)\n",
    "- [SVM Wrapup](SVM_Coda.ipynb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 7\n",
    "\n",
    "**Plan** \n",
    "- Principal Components: continued\n",
    "- Gradient Descent: how to minimize a Loss function\n",
    "- Interpretation: understanding models\n",
    "\n",
    "**Principal Components (continued)**\n",
    "- [linked notebook](Unsupervised.ipynb#Example:-Reconstructing-$\\x$-from-$\\tilde\\x$-and-the-principal-components)\n",
    "\n",
    "**Gradient Descent** \n",
    "- [Gradient Descent](Gradient_Descent.ipynb)\n",
    "\n",
    "**Interpretation**\n",
    "- [Interpretation: Linear Models](Linear_Model_Interpretation.ipynb)\n",
    "  \n",
    "**Deeper Dives**\n",
    "- [Missing data: clever ways to impute values](Missing_Data.ipynb)\n",
    "- [Other matrix factorization methods](Unsupervised_Other_Factorizations.ipynb)\n",
    "- [Feature importance](Feature_Importance.ipynb)\n",
    "- [SVC Loss function derivation](SVM_Derivation.ipynb)\n",
    "\n",
    "\n",
    "## Deep Learning: Headstart\n",
    "\n",
    "### Week 1 Introduction to Neural Networks and Deep Learning\n",
    "\n",
    "**Plan**\n",
    "\n",
    "Deep Learning/Neural networks\n",
    "\n",
    "- [Set up your Tensorflow environment](Tensorflow_setup.ipynb)\n",
    "- [Neural Networks Overview](Neural_Networks_Overview.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<html>\n",
    "<p style=\"font-size:32px\"><strong>Deep Learning</strong></p>\n",
    "</html>\n",
    "\n",
    "# Week 1 Introduction to Neural Networks and Deep Learning\n",
    "\n",
    "\n",
    "**Review of Midterm Project**\n",
    "\n",
    "\n",
    "**Plan**\n",
    "\n",
    "Continue our introduction to Neural Networks/Deep Learning\n",
    "\n",
    "- Neural network overview\n",
    "- Neural network: practical\n",
    "- Neural network theory\n",
    "\n",
    "[Neural Nework Overview (continued)](Neural_Networks_Overview.ipynb#What-is-$W_\\llp$-?-Where-did-$\\Theta$-go-?)\n",
    "\n",
    "**Neural network: practical**\n",
    "- Coding Neural Networks: Tensorflow, Keras\n",
    "    - [Intro to Keras](Tensorflow_Keras.ipynb)\n",
    "\n",
    "- Practical Colab\n",
    "<!--- The Colab notebook imports some modules; make sure they are in the repo --->\n",
    "<!--- #include (neural_net_helper.py) --->\n",
    "<!--- The Colab notebook imports some modules; make sure they are in the repo --->\n",
    "<!--- #include (Colab_practical.ipynb)) --->\n",
    "   - **Colab**: [Practical Colab Notebook from github](https://colab.research.google.com/github/kenperry-public/ML_Spring_2023/blob/master/Colab_practical.ipynb)\n",
    " \n",
    "**Neural network theory**\n",
    "- [A neural network is a Universal Function Approximator](Universal_Function_Approximator.ipynb)\n",
    "\n",
    "**Training Neural Networks (introduction)**\n",
    "- [Training Neural Networks - Back propagation](Training_Neural_Network_Backprop.ipynb)\n",
    "\n",
    "   \n",
    "**Deeper Dives**\n",
    "<!--- #include (Raw_TensorFlow.ipynb)) --->\n",
    "- [Keras, from past to present](Tensorflow_Keras_Archaeology.ipynb)\n",
    "- [History/Computation Graphs: Tensorflow version 1](DNN_TensorFlow_Using_TF_version_1.ipynb)\n",
    "- [Raw_TensorFlow example Notebook from github](https://colab.research.google.com/github/kenperry-public/ML_Spring_2023/blob/master/Raw_TensorFlow.ipynb) (**Colab**)\n",
    "- [Computation Graphs](Computation_Graphs.ipynb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 2 Convolutional Neural Networks\n",
    "\n",
    "\n",
    "**Plan**\n",
    "- Automatic computation of gradients\n",
    "- The Convolutional Neural Network layer type\n",
    "- We start to answer the question posed in our introduction to Neural Networks:\n",
    ">What took so long ?\n",
    "- Why training a Neural Network can be difficult: fine-details of training\n",
    "\n",
    "**How to compute gradients automatically**\n",
    "- [Why TensorFlow ?: Gradients made easy](Training_Neural_Network_Operation_Forward_and_Backward_Pass.ipynb)\n",
    "\n",
    "\n",
    "\n",
    "**Convolutional Neural Networks (CNN)**\n",
    "- [Introduction to CNN](Intro_to_CNN.ipynb)\n",
    "- [CNN: multiple input/output features](CNN_Overview.ipynb)\n",
    "    - [CNN pictorial](CNN_pictorial.ipynb)\n",
    "- [CNN: Space and Time](CNN_Space_and_Time.ipynb)\n",
    "    - [CNN example from github](https://colab.research.google.com/github/kenperry-public/ML_Spring_2023/blob/master/CNN_demo.ipynb) (**Colab**) \n",
    "    - [CNN example from github](CNN_demo.ipynb) (**local machine**) \n",
    "\n",
    "**Training Neural Networks: the fine details**\n",
    "- [Training Neural Networks: fine details (continued)](Training_Neural_Networks_Overview.ipynb)\n",
    "\n",
    "**Deeper dives**\n",
    "- [Convolution as Matrix Multiplication](CNN_Convolution_as_Matrix_Multiplication.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 3 Recurrent Neural Networks\n",
    "\n",
    "**Plan**\n",
    "- Why training a Neural Network can be difficult: fine-details of training\n",
    "- Introduce a new layer type: Recurrent layers\n",
    "    - Part of our \"sprint\": final layer type\n",
    "    - Will revisit more theoretical issues in subsequent lectures\n",
    "\n",
    "\n",
    "**Training Neural Networks: the fine details**\n",
    "- [Training Neural Networks: fine details (continued)](Training_Neural_Networks_Overview.ipynb#Initializing-and-maintaining-layer-inputs)\n",
    "\n",
    "\n",
    "**Recurrent Neural Networks (RNN)**\n",
    "- [Introduction to Recurrent Neural Network (RNN)](Intro_to_RNN.ipynb)\n",
    "- [Recurrent Neural Network Overview](RNN_Overview.ipynb)\n",
    "- [Using an RNN for Generative AI](RNN_generative.ipynb)\n",
    "    - [LSTM_text_generation from github](https://colab.research.google.com/github/kenperry-public/ML_Spring_2023/blob/master/Keras_examples_LSTM_text_generation.ipynb) (**Colab**)\n",
    "    - [LSTM_text_generation from github](Keras_examples_LSTM_text_generation.ipynb) (**local machine**)\n",
    "\n",
    "**Language Model: a sequence task**\n",
    "- [Language model demo](https://app.inferkit.com/demo)\n",
    "\n",
    "**RNN: Issues**\n",
    "- [Gradients of an RNN](RNN_Gradients.ipynb)\n",
    "- [RNN: Gradients that Vanish/Explode](RNN_Vanishing_and_exploding_gradients.ipynb)\n",
    "\n",
    "Sprint is over ! We have covered the basic layer types; time for you to learn by experimenting.\n",
    "\n",
    "**Review of layer types**\n",
    "- [What layer type to choose](Neural_Network_Layer_Review.ipynb)\n",
    "\n",
    "**Deeper dives**\n",
    "- [RNN: How to deal with long sequences](RNN_Long_Sequences.ipynb)\n",
    "- [Tensors: Matrix gradients](Matrix_Gradient.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 4 Advanced Recurrent Architectures; Transfer Learning\n",
    "\n",
    "**Plan**\n",
    "\n",
    "The \"vanilla\" Recurrent Neural Network (RNN) layer we learned is very much exposed to the problem of vanishing/exploding gradients.\n",
    "\n",
    "We will review the issue and demonstrate a related layer type (the LSTM) designed to mitigate the problem.\n",
    "\n",
    "We  present an extremely useful trick (Transfer Learning) for leveraging the hard work that others have done.\n",
    "\n",
    "**Concepts**\n",
    "\n",
    "There are a number of pieces of the LSTM which can appear overwhelming when seen together for the first time.  We will explore these concepts separately before seeing how they are integrated into the LSTM.\n",
    "\n",
    "- [Residual connections](RNN_Residual_Networks.ipynb)\n",
    "- [Neural Programming](Neural_Programming.ipynb)\n",
    "\n",
    "**LSTM: An improved RNN**\n",
    "\n",
    "- [Introduction to the LSTM](Intro_to_LSTM.ipynb)\n",
    "- [LSTM Overview](LSTM_Overview.ipynb)\n",
    "\n",
    "**Transfer Learning**\n",
    "\n",
    "Transfer learning allows us to adapt a model trained for one task to be able to solve a new task with a small amount of work.  As models get bigger and bigger, the future of Deep Learning may be one where you use Transfer Learning more than developing your own models from scratch.\n",
    "\n",
    "- [Transfer Learning](Transfer_Learning.ipynb)\n",
    "\n",
    "     - [Transfer Learning example from github](https://colab.research.google.com/github/kenperry-public/ML_Spring_2023/blob/master/TransferLearning_demo.ipynb) (**Colab**)\n",
    "     - [Transfer Learning example from github](TransferLearning_demo.ipynb) (**local machine**)\n",
    "\n",
    "     - [Utility notebook](Dogs_and_Cats_reformat.ipynb)\n",
    "         - Takes the *very large* raw data (from Kaggle) used in the Transfer Learning example\n",
    "         - Creates a much smaller subset, using a different directory structure\n",
    "         - The above notebook uses this reorganized, smaller subset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 5 Transformers\n",
    "\n",
    "**Plan**\n",
    "\n",
    "We present Attention, a way to enhance the power of RNN's, which is heavily used in a new layer type for sequence processing: the Transformer.  \n",
    "\n",
    "The Transformer layer type is now predominant in the area of Natural Language Processing (NLP).\n",
    "We give a quick introduction but we will revisit it in the module on advanced NLP.\n",
    "\n",
    "\n",
    "**Attention**\n",
    "- [Attention (continued)](Intro_to_Attention.ipynb#Self-attention)\n",
    "- [Implementing Attention](Attention_Lookup.ipynb)\n",
    "   \n",
    "**Transformers**\n",
    "- [Transformer](Transformer.ipynb)\n",
    "\n",
    "\n",
    "\n",
    "**Deeper dives**\n",
    "- Transformer\n",
    "    - [Keras example: pre-defined Attention layers](https://keras.io/examples/nlp/text_classification_with_transformer/)\n",
    "        - [notebook](https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/nlp/ipynb/text_classification_with_transformer.ipynb)\n",
    "    - [TensorFlow tutorial: implements Attention, positional encoding](https://www.tensorflow.org/text/tutorials/transformer)\n",
    "       - [notebook](https://colab.research.google.com/github/tensorflow/text/blob/master/docs/tutorials/transformer.ipynb)\n",
    "\n",
    "\n",
    "\n",
    "**Further reading**\n",
    "- Attention\n",
    "    - [Neural Machine Translation by Jointly Learning To Align and Translate](https://arxiv.org/pdf/1409.0473.pdf)\n",
    "    - [Attention is all you need](https://arxiv.org/pdf/1706.03762.pdf)\n",
    "- Transfer Learning    \n",
    "    - [Sebastian Ruder: Transfer Learning](https://ruder.io/transfer-learning/)\n",
    "\n",
    "## Early start: NLP\n",
    "\n",
    "**Learning from text: Deep Learning for Natural Language Processing (NLP)**\n",
    "- [Natural Language Processing Overview](NLP_Overview.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 6 NLP; Language Models\n",
    "\n",
    "**Hyper-parameter search: Keras tuner**\n",
    "\n",
    "- [Keras tuner example](https://colab.research.google.com/github/kenperry-public/ML_Spring_2023/blob/master/DNN_TensorFlow_example.ipynb) (**Colab**) \n",
    "- [Kera tuner example](DNN_TensorFlow_example.ipynb) (**local machine**) \n",
    "\n",
    "\n",
    "**Plan**\n",
    "\n",
    "(NLP)\n",
    "\n",
    "We will make an initial pass on the topic of learning from text: Natural Language Processing.\n",
    "\n",
    "The first pass will use well-established techniques that are relatively easy to follow.\n",
    "\n",
    "We then explore some recent advances that have greatly increased the power of NLP.\n",
    "\n",
    "The Transformer architecture is a key contributor.\n",
    "\n",
    "\n",
    "**Learning from text: Deep Learning for Natural Language Processing (NLP)**\n",
    "\n",
    "- [Issues with Text (continued)](NLP_Tokens.ipynb#Issues-with-text:-variable-length-sequences)\n",
    "- [Natural Language Processing Overview (continued)](NLP_Overview.ipynb#Sentiment-classification-notebook-on-Colab:-simple-model)\n",
    "    - [NLP from github (Colab)](https://colab.research.google.com/github/kenperry-public/ML_Spring_2023/blob/master/Keras_examples_imdb_cnn.ipynb)\n",
    "    - [NLP from github (local machine)](Keras_examples_imdb_cnn.ipynb)\n",
    "   \n",
    "<!--- #include (squad_show.csv) --->\n",
    "\n",
    "**Evolution of Word representations**\n",
    "- [How to represent a word: syntax](NLP_Tokenization.ipynb)\n",
    "- [How to represent a word: meaning](NLP_Word_Representations.ipynb)\n",
    "\n",
    "## Week 7 Early Start: Language Models\n",
    "\n",
    "**Language Models: the future (present ?) of NLP ?**\n",
    "\n",
    "- [Language Models](NLP_Language_Models.ipynb)\n",
    "- [Large Language Models](NLP_Large_Language_Models.ipynb)\n",
    "\n",
    "<!--- #include (squad_show.csv) --->\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 7 Advanced Topics\n",
    "\n",
    "**Plan**  \n",
    "\n",
    "(NLP)\n",
    "\n",
    "We conclude the module on Large Language Models with the question:\n",
    "> Can a model trained for one task *also* solve other tasks, including ones it has not seen in training ?\n",
    "\n",
    "**Language Models: the future (present ?) of NLP ? (continued)**\n",
    "\n",
    "- [Large Language Models (continued)](NLP_Large_Language_Models.ipynb#GPT-3)\n",
    "- [In-Context Learning](In_Context_Learning.ipynb)\n",
    "- [PreTrain_Prompt_Predict](NLP_Beyond_LLM.ipynb)\n",
    "- [From LLM to Bing Search](From_GPT_to_BingSearch.ipynb)\n",
    "\n",
    "<!--- #include (squad_show.csv) --->\n",
    "\n",
    "\n",
    "(Advanced Keras)\n",
    "\n",
    "It turns out that we can't implement these models using the Sequential model of Keras that we have been\n",
    "using all semester:\n",
    "- it's time to give a peek at \n",
    "the Functional model.\n",
    "\n",
    "We look at more advanced Keras techniques that allow us to write complex Loss functions.\n",
    "- Additionally, we will look at other advanced features such as\n",
    "    - Custom training loop\n",
    "    - Cool things you can do with Gradients\n",
    "    - Defining your own layer type\n",
    "\n",
    "We introduce a technique called *Gradient Ascent*. \n",
    "\n",
    "This will be useful both to illustrate Advanced Keras but also to introduce a tool for interpretation.\n",
    "\n",
    "**Advanced Keras**\n",
    "- [Advanced Keras](Keras_Advanced.ipynb)\n",
    "- [Gradient Ascent](Gradient_ascent.ipynb)\n",
    "    - Useful for Interpretaton\n",
    "\n",
    "(Interpretation)\n",
    "\n",
    "As we've hinted at before: the art of Deep Learning is writing a Loss function that captures the semantics of the problem you want to solve.  \n",
    "\n",
    "-\n",
    "We've spent several weeks learning about Neural Networks.\n",
    "At best, we suggested theories for how they are able to achieve what they do.\n",
    "\n",
    "This week: we will explore methods for testing theories as to how Neural Networks work.\n",
    "\n",
    "\n",
    "**What is a Neural Network really doing ? Interpretation**\n",
    "- [Interpretation: preview](Interpretation_preview.ipynb)\n",
    "- [Introduction to Interpretation of Deep Learning](Intro_to_Interpretation_of_DL.ipynb)\n",
    "- [Interpretation: Simple Methods](Interpretation_of_DL_Simple.ipynb)\n",
    "- [Interpretation: Saliency Maps](Interpretation_of_DL_Deconv.ipynb)\n",
    "- [Interpretation: Gradient Ascent](Interpretation_of_DL_Gradient_Ascent.ipynb)\n",
    "- [Adversarial Examples](Adversarial_Examples.ipynb)\n",
    "\n",
    "**Wrapping up**\n",
    "- [Final thoughts](Deep_Learning_Coda.ipynb)\n",
    "\n",
    "\n",
    "**Topics we probably won't have time to cover**\n",
    "\n",
    "(Generative ML)\n",
    "\n",
    "We revisit the topic of Generative (as opposed to Discriminative) ML through our study of Autoencoders\n",
    "and Generative Adversarial Networks (GANs).\n",
    "\n",
    "- Generative ML not only allows us to create fun things (like stories and images) but also\n",
    "is a way of creating *synthetic data* which which to augment small training data sets.\n",
    "\n",
    "- (We will defer examining the code references to the module on Advanced Keras)\n",
    "\n",
    "**Generative ML**\n",
    "- [Autoencoders](Autoencoders_and_Generative_Models.ipynb)\n",
    "- [Generative Adversarial Networks](GAN_Overview.ipynb)\n",
    "- [Neural Style Transfer](Neural_Style_Transfer.ipynb)\n",
    "\n",
    "**Deeper Dives**\n",
    "- [Factor Models and Autoencoders](Autoencoder_for_conditional_risk_factors.ipynb)\n",
    "- [Interpretation using Principal Components](Interpretation_of_DL_Simple_PCA.ipynb)\n",
    "- [Interpretation: Attention](Interpretation_of_DL_Attention.ipynb)\n",
    "\n",
    "\n",
    "**Further reading**\n",
    "- [Tutorial on Variational Autoencoders](https://arxiv.org/pdf/1606.05908.pdf)\n",
    "- [From GAN to WGAN](https://lilianweng.github.io/posts/2017-08-20-gan/)\n",
    "- [Wasserstein GAN](https://www.alexirpan.com/2017/02/22/wasserstein-gan.html)\n",
    "- [Visualizing and Understanding Convolutional Networks](https://arxiv.org/pdf/1311.2901.pdf)\n",
    "- [Old blog: How CNN's see the world](https://blog.keras.io/how-convolutional-neural-networks-see-the-world.html)\n",
    "    - **Note**: the code is obsolete, but the discussion is very good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Assignments\n",
    "\n",
    "Your assignments should follow the [Assignment Guidelines](assignments/Assignment_Guidelines.ipynb)\n",
    "\n",
    "\n",
    "\n",
    "## Regression\n",
    "- Assignment notebook: [Using Machine Learning for Hedging](assignments/Regression%20task/Using_Machine_Learning_for_Hedging.ipynb)\n",
    "- Data\n",
    "    - There is an archive file containing the data\n",
    "    - You can find it\n",
    "        - Under the course page: Content --> Data --> Assignments --> Regression task\n",
    "        - You won't be able to view the file in the browser, but you **will** be able to Download it\n",
    "    - You should unzip this archive into the *the same directory* as the assignment notebook\n",
    "    - The end result is that the directory should contain\n",
    "        - The assignment notebook and a helper file\n",
    "        - A directory named `Data`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classification\n",
    "- Assignment notebook: [Ships in satellite images](assignments/Classification%20task/Ships_in_satellite_images.ipynb#)\n",
    "- Data\n",
    "    - There is an archive file containing the data\n",
    "    - You can find it\n",
    "        - Under the course page: Content --> Data --> Assignments --> Classification task\n",
    "        - You won't be able to view the file in the browser, but you **will** be able to Download it\n",
    "    - You should unzip this archive into the *the same directory* as the assignment notebook\n",
    "    - The end result is that the directory should contain\n",
    "        - The assignment notebook and a helper file\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Midterm Project: Bankruptcy One Year Ahead\n",
    "- Assignment notebook [Bankruptcy One Year Ahead](assignments/bankruptcy_one_yr/Bankruptcy_oya.ipynb)\n",
    "- Data\n",
    "    - There is an archive file containing the data\n",
    "    - You can find it\n",
    "        - Under the course page: Content --> Data --> Assignments --> Bankruptcy One Year Ahead\n",
    "        - You won't be able to view the file in the browser, but you **will** be able to Download it\n",
    "    - You should unzip this archive into the *the same directory* as the assignment notebook\n",
    "    - The end result is that the directory should contain\n",
    "        - The assignment notebook and a helper file\n",
    "        - A directory named `Data`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Keras practice\n",
    "- Assignment notebook [Ships in satellite images: Neural Network](assignments/keras_intro/Ships_in_satellite_images_P1.ipynb)\n",
    "- Data (same as for the Classification assignment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Convolutional Neural Networks (CNN)\n",
    "- Assignment notebook [Ships in satellite images: Neural Network](assignments/CNN_intro/Ships_in_satellite_images_P2.ipynb)\n",
    "- Data (same as for the Classification assignment)\n",
    "    - please repeat the directions given in that assignment for obtaining the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Final project; Stock prediction\n",
    "\n",
    " - Assignment notebooks:\n",
    "    - [Stock prediction](assignments/stock_prediction/Final_project_StockPrediction.ipynb)\n",
    "    - [Submission guidelines](assignments/stock_prediction/Final_project.ipynb)\n",
    "   \n",
    " - Data\n",
    "    - There is an archive file containing the data\n",
    "    - You can find it\n",
    "        - Under the course page: Content --> Data --> Assignments --> Stock Prediction\n",
    "        - You won't be able to view the file in the browser, but you **will** be able to Download it\n",
    "    - You should unzip this archive into the *the same directory* as the assignment notebook\n",
    "    - The end result is that the directory should contain\n",
    "        - The assignment notebook, submission guidelines notebook\n",
    "        - A directory named `Data/train`\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "369.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
